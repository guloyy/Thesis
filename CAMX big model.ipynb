{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is purely for the calculation of daily averages from the merged parquet file. Keep this just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876cff80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ParquetSchema object at 0x7f95e8433140>\n",
      "required group field_id=-1 schema {\n",
      "  optional int64 field_id=-1 Datetime (Timestamp(isAdjustedToUTC=false, timeUnit=microseconds, is_from_converted_type=false, force_set_converted_type=false));\n",
      "  optional binary field_id=-1 site_name (String);\n",
      "  optional binary field_id=-1 station_id (String);\n",
      "  optional double field_id=-1 site_latitude;\n",
      "  optional double field_id=-1 site_longitude;\n",
      "  optional double field_id=-1 NO3;\n",
      "  optional double field_id=-1 SO4;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Replace the string with the path to your file\n",
    "file_path = \"/Users/guloy/Desktop/CAMX model data/merged_data.parquet\"\n",
    "\n",
    "# Read the Parquet file\n",
    "parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "# Print the schema\n",
    "print(parquet_file.schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "974d67a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file created at: /Users/guloy/Desktop/CAMX model data/merged_data.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the directory where your parquet files are located\n",
    "directory = '/Users/guloy/Desktop/CAMX model data'\n",
    "files = [f for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over the files to read them into pandas DataFrames\n",
    "for file in files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_parquet(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into one\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Write the merged DataFrame to a new Parquet file\n",
    "merged_file_path = os.path.join(directory, 'merged_data.parquet')\n",
    "merged_df.to_parquet(merged_file_path)\n",
    "\n",
    "print(f'Merged file created at: {merged_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f35d5dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['camx_NO3_SO4_Allsites_2017.parquet', 'camx_NO3_SO4_Allsites_2015.parquet', 'camx_NO3_SO4_Allsites_2011.parquet', 'camx_NO3_SO4_Allsites_2019.parquet', 'camx_NO3_SO4_Allsites_2013.parquet']\n"
     ]
    }
   ],
   "source": [
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "951438a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Datetime site_name station_id  site_latitude  site_longitude  \\\n",
      "0  2017-01-01 00:00:00     Puijo      ID001         62.902           27.65   \n",
      "1  2017-01-01 01:00:00     Puijo      ID001         62.902           27.65   \n",
      "2  2017-01-01 02:00:00     Puijo      ID001         62.902           27.65   \n",
      "3  2017-01-01 03:00:00     Puijo      ID001         62.902           27.65   \n",
      "4  2017-01-01 04:00:00     Puijo      ID001         62.902           27.65   \n",
      "5  2017-01-01 05:00:00     Puijo      ID001         62.902           27.65   \n",
      "6  2017-01-01 06:00:00     Puijo      ID001         62.902           27.65   \n",
      "7  2017-01-01 07:00:00     Puijo      ID001         62.902           27.65   \n",
      "8  2017-01-01 08:00:00     Puijo      ID001         62.902           27.65   \n",
      "9  2017-01-01 09:00:00     Puijo      ID001         62.902           27.65   \n",
      "10 2017-01-01 10:00:00     Puijo      ID001         62.902           27.65   \n",
      "11 2017-01-01 11:00:00     Puijo      ID001         62.902           27.65   \n",
      "12 2017-01-01 12:00:00     Puijo      ID001         62.902           27.65   \n",
      "13 2017-01-01 13:00:00     Puijo      ID001         62.902           27.65   \n",
      "14 2017-01-01 14:00:00     Puijo      ID001         62.902           27.65   \n",
      "15 2017-01-01 15:00:00     Puijo      ID001         62.902           27.65   \n",
      "16 2017-01-01 16:00:00     Puijo      ID001         62.902           27.65   \n",
      "17 2017-01-01 17:00:00     Puijo      ID001         62.902           27.65   \n",
      "18 2017-01-01 18:00:00     Puijo      ID001         62.902           27.65   \n",
      "19 2017-01-01 19:00:00     Puijo      ID001         62.902           27.65   \n",
      "20 2017-01-01 20:00:00     Puijo      ID001         62.902           27.65   \n",
      "21 2017-01-01 21:00:00     Puijo      ID001         62.902           27.65   \n",
      "22 2017-01-01 22:00:00     Puijo      ID001         62.902           27.65   \n",
      "23 2017-01-01 23:00:00     Puijo      ID001         62.902           27.65   \n",
      "24 2017-01-02 00:00:00     Puijo      ID001         62.902           27.65   \n",
      "\n",
      "         NO3       SO4  \n",
      "0   0.336923  0.039437  \n",
      "1   0.465504  0.042260  \n",
      "2   0.474997  0.043833  \n",
      "3   0.445169  0.044146  \n",
      "4   0.429346  0.044416  \n",
      "5   0.453717  0.044384  \n",
      "6   0.515770  0.044335  \n",
      "7   0.632943  0.050190  \n",
      "8   0.716081  0.058522  \n",
      "9   0.708101  0.062857  \n",
      "10  0.662206  0.069958  \n",
      "11  0.615209  0.078137  \n",
      "12  0.584976  0.086736  \n",
      "13  0.588171  0.094470  \n",
      "14  0.620610  0.099673  \n",
      "15  0.683387  0.106169  \n",
      "16  0.782283  0.112218  \n",
      "17  0.910116  0.111632  \n",
      "18  0.997083  0.104308  \n",
      "19  0.950447  0.097594  \n",
      "20  0.789256  0.083139  \n",
      "21  0.585313  0.069177  \n",
      "22  0.403240  0.108846  \n",
      "23  0.260176  0.155904  \n",
      "24  0.136855  0.202727  \n",
      "     Datetime  site_latitude  site_longitude       NO3       SO4\n",
      "0  2011-01-01      49.607112         9.40791  7.238929  4.325453\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the merged Parquet file\n",
    "merged_file_path = '/Users/guloy/Desktop/CAMX model data/merged_data.parquet'\n",
    "\n",
    "# Read the merged Parquet file\n",
    "merged_df = pd.read_parquet(merged_file_path)\n",
    "merged_df_average= pd.read_parquet(\"/Users/guloy/Desktop/CAMX model data/daily_averages.parquet\")\n",
    "# Print the first three rows of the DataFrame\n",
    "print(merged_df.head(25))\n",
    "print(merged_df_average.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b57bc8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily averages file created at: /Users/guloy/Desktop/CAMX model data/daily_averages.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the merged Parquet file\n",
    "merged_file_path = '/Users/guloy/Desktop/CAMX model data/merged_data.parquet'\n",
    "\n",
    "# Read the merged Parquet file\n",
    "merged_df = pd.read_parquet(merged_file_path)\n",
    "\n",
    "# Ensure the DateTime column is in datetime64 format\n",
    "merged_df['Datetime'] = pd.to_datetime(merged_df['Datetime'])\n",
    "\n",
    "# Select only numeric columns for aggregation, excluding columns of type 'object' (string)\n",
    "numeric_cols = merged_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Group by the date and take the mean for each day for numeric columns only\n",
    "daily_avg_df = merged_df.groupby(merged_df['Datetime'].dt.date)[numeric_cols].mean()\n",
    "\n",
    "# Reset the index to get the date as a column\n",
    "daily_avg_df.reset_index(inplace=True)\n",
    "\n",
    "# Rename the 'index' column to 'Date' to reflect that it's now just a date\n",
    "daily_avg_df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "# Write the resulting DataFrame with daily averages back to a new Parquet file\n",
    "daily_avg_file_path = '/Users/guloy/Desktop/CAMX model data/daily_averages.parquet'\n",
    "daily_avg_df.to_parquet(daily_avg_file_path)\n",
    "\n",
    "print(f'Daily averages file created at: {daily_avg_file_path}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb0f0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('/Users/guloy/Desktop/CAMX model data/merged_data.parquet')\n",
    "\n",
    "# Convert 'Datetime' to datetime type and extract the date\n",
    "df['Date'] = df['Datetime'].dt.date\n",
    "\n",
    "# Group by 'station_id' and 'Date' and calculate the mean of 'NO3' and 'SO4'\n",
    "# Also, assume that 'site_name', 'site_latitude', and 'site_longitude' are constant per 'station_id'\n",
    "aggregated_df = df.groupby(['station_id', 'Date']).agg({\n",
    "    'NO3': 'mean',\n",
    "    'SO4': 'mean',\n",
    "    'site_name': 'first',  # or 'last' if you are sure they are the same for each 'station_id'\n",
    "    'site_latitude': 'first',\n",
    "    'site_longitude': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Now, the 'aggregated_df' contains the average values of 'NO3' and 'SO4' for each date and 'station_id'\n",
    "# along with the corresponding 'site_name', 'site_latitude', and 'site_longitude'.\n",
    "\n",
    "# Save the new DataFrame to a Parquet file if needed\n",
    "aggregated_df.to_parquet('/Users/guloy/Desktop/CAMX model data/daily_averages.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e4c6af49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        station_id        Date        NO3       SO4       site_name  \\\n",
      "0            ID001  2011-01-01   2.191915  0.386462           Puijo   \n",
      "1            ID001  2011-01-02   3.122906  0.267743           Puijo   \n",
      "2            ID001  2011-01-03   5.457647  0.359734           Puijo   \n",
      "3            ID001  2011-01-04   5.214429  0.330329           Puijo   \n",
      "4            ID001  2011-01-05   5.044306  0.576013           Puijo   \n",
      "...            ...         ...        ...       ...             ...   \n",
      "1465470      ID824  2019-12-27   3.161087  0.678170  Zurich-Kaserne   \n",
      "1465471      ID824  2019-12-28  12.906357  3.013269  Zurich-Kaserne   \n",
      "1465472      ID824  2019-12-29   4.638141  0.593835  Zurich-Kaserne   \n",
      "1465473      ID824  2019-12-30   2.997853  0.145686  Zurich-Kaserne   \n",
      "1465474      ID824  2019-12-31   5.218395  0.306131  Zurich-Kaserne   \n",
      "\n",
      "         site_latitude  site_longitude  \n",
      "0               62.902          27.650  \n",
      "1               62.902          27.650  \n",
      "2               62.902          27.650  \n",
      "3               62.902          27.650  \n",
      "4               62.902          27.650  \n",
      "...                ...             ...  \n",
      "1465470         47.378           8.531  \n",
      "1465471         47.378           8.531  \n",
      "1465472         47.378           8.531  \n",
      "1465473         47.378           8.531  \n",
      "1465474         47.378           8.531  \n",
      "\n",
      "[1465475 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure the path reflects the renamed .parquet file\n",
    "df = pd.read_parquet('/Users/guloy/Desktop/CAMX model data/daily_averages.parquet')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a17e0f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  station_id        Date       NO3       SO4 site_name  site_latitude  \\\n",
      "0      ID001  2011-01-01  2.191915  0.386462     Puijo         62.902   \n",
      "1      ID001  2011-01-02  3.122906  0.267743     Puijo         62.902   \n",
      "2      ID001  2011-01-03  5.457647  0.359734     Puijo         62.902   \n",
      "3      ID001  2011-01-04  5.214429  0.330329     Puijo         62.902   \n",
      "4      ID001  2011-01-05  5.044306  0.576013     Puijo         62.902   \n",
      "\n",
      "   site_longitude  \n",
      "0           27.65  \n",
      "1           27.65  \n",
      "2           27.65  \n",
      "3           27.65  \n",
      "4           27.65  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/guloy/Desktop/CAMX model data/daily_averages.parquet'  # Assuming the extension is just a naming issue\n",
    "df = pd.read_parquet(file_path)\n",
    "\n",
    "print(df.head())  # This will print the first few rows of the dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from here we are processing model + excel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e1ddacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the various Excel sheets\n",
    "italy_consolidated_path = '/Users/guloy/Desktop/EDITED REPO/ITALY_CONSOLIDATED.xlsx'\n",
    "milan_pascal_pm10 = pd.read_excel(italy_consolidated_path, sheet_name='MILAN_PASCAL_PM10')\n",
    "milan_pascal_25 = pd.read_excel(italy_consolidated_path, sheet_name='MILAN_PASCAL_PM25')\n",
    "\n",
    "italy_2_consolidated_path = '/Users/guloy/Desktop/EDITED REPO/ITALY_2_CONSOLIDATED.xlsx'\n",
    "milano_pascal_all = pd.read_excel(italy_2_consolidated_path, sheet_name='MILANO_PASCAL_ALL')\n",
    "\n",
    "milano_obs_path = '/Users/guloy/Desktop/milano_obs_camx_road_data_df_RF.xlsx'\n",
    "milano_obs = pd.read_excel(milano_obs_path)\n",
    "\n",
    "# Convert date columns to datetime if they are not already\n",
    "milano_obs['Datetime'] = pd.to_datetime(milano_obs['Datetime'])\n",
    "milan_pascal_pm10['Date'] = pd.to_datetime(milan_pascal_pm10['Date'])\n",
    "milan_pascal_25['Date'] = pd.to_datetime(milan_pascal_25['Date'])\n",
    "milano_pascal_all['Date'] = pd.to_datetime(milano_pascal_all['Date'])\n",
    "\n",
    "# Perform the left joins\n",
    "join1 = pd.merge(milano_obs, milan_pascal_pm10[['Date', 'NO3-', 'SO42-']], left_on='Datetime', right_on='Date', how='left').drop('Date', axis=1)\n",
    "join2 = pd.merge(milano_obs, milan_pascal_25[['Date', 'NO3-', 'SO42-']], left_on='Datetime', right_on='Date', how='left').drop('Date', axis=1)\n",
    "join3 = pd.merge(milano_obs, milano_pascal_all[['Date', 'NO3-', 'SO42-']], left_on='Datetime', right_on='Date', how='left').drop('Date', axis=1)\n",
    "\n",
    "# Combine the results if necessary\n",
    "# This step depends on how you want to organize the final data. \n",
    "# For instance, you can concatenate them or keep them separate.\n",
    "\n",
    "# Example of concatenation\n",
    "final_df = pd.concat([join1, join2, join3])\n",
    "\n",
    "# Save the final result to a new Excel file\n",
    "final_df.to_excel('/Users/guloy/Desktop/final_output.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7725487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564db79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
