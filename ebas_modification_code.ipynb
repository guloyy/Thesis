{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ffdc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the files are named 'file1.csv' and 'file2.csv'\n",
    "# Replace these with the actual file paths\n",
    "file_path2 = '/Users/guloy/Desktop/EBAS_obs_processed/NITRATE_EBAS_all_df.csv'\n",
    "file_path2 = '/Users/guloy/Desktop/EBAS_obs_processed/SULFATE_EBAS_all_df.csv'\n",
    "\n",
    "# Read the first CSV file with semicolon as the delimiter\n",
    "df1 = pd.read_csv(file_path1, delimiter=';')\n",
    "df2 = pd.read_csv(file_path2)\n",
    "# Read the second CSV file (assuming comma is the delimiter)\n",
    "#df2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Rename the 'time' column to 'Date' in both dataframes\n",
    "df1.rename(columns={'time': 'Date'}, inplace=True)\n",
    "df2.rename(columns={'time': 'Date'}, inplace=True)\n",
    "\n",
    "# Rename the 'time' column to 'Date' in both dataframes\n",
    "df1.rename(columns={'station_id': 'ebas_id'}, inplace=True)\n",
    "df2.rename(columns={'station_id': 'ebas_id'}, inplace=True)\n",
    "\n",
    "# Change the date format in the 'Date' column to 'YYYY-MM-DD'\n",
    "df1['Date'] = pd.to_datetime(df1['Date']).dt.strftime('%Y-%m-%d')\n",
    "df2['Date'] = pd.to_datetime(df2['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "df1.drop(df1.columns[0], axis=1, inplace=True)\n",
    "\n",
    "df2.drop(df2.columns[0], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#df1.drop('station_name', axis=1, inplace=True)\n",
    "#df2.drop('station_name', axis=1, inplace=True)\n",
    "\n",
    "df1.drop('matrix', axis=1, inplace=True)\n",
    "df2.drop('matrix', axis=1, inplace=True)\n",
    "\n",
    "df1.drop('file', axis=1, inplace=True)\n",
    "df2.drop('file', axis=1, inplace=True)\n",
    "\n",
    "df1.drop('method_ref', axis=1, inplace=True)\n",
    "df2.drop('method_ref', axis=1, inplace=True)\n",
    "\n",
    "df1.drop('instrument_type', axis=1, inplace=True)\n",
    "df2.drop('instrument_type', axis=1, inplace=True)\n",
    "\n",
    "df1.rename(columns={'nitrate': 'NO3-'}, inplace=True)\n",
    "df2.rename(columns={'sulphate_total': 'SO42-'}, inplace=True)\n",
    "\n",
    "# Assuming you've already done your modifications (like dropping columns) to df1 and df2\n",
    "\n",
    "# Save the modified DataFrame df1 back to a CSV file\n",
    "df1.to_csv('/Users/guloy/Desktop/modified_file1.csv', index=False)\n",
    "\n",
    "# Save the modified DataFrame df2 back to a CSV file\n",
    "df2.to_csv('/Users/guloy/Desktop/modified_file2.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03d8e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in second file: Index(['Date', 'SO42-', 'Lat', 'Lon', 'station_id'], dtype='object')\n",
      "Columns in third file: Index(['Date', 'NO3-', 'Lat', 'Lon', 'station_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming the files are named 'file1.csv' and 'file2.csv'\n",
    "# Replace these with the actual file paths\n",
    "file_path1 = '/Users/guloy/Desktop/modified_file1.csv'\n",
    "file_path2 = '/Users/guloy/Desktop/modified_file2.csv'\n",
    "#file_path3 = '/Users/guloy/Desktop/EBAS_obs_processed/SULFATE_EBAS_all_df.csv'\n",
    "# Read the CSV files into pandas dataframes\n",
    "#df1 = pd.read_csv(file_path1)\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "\n",
    "\n",
    "# Print column names to verify\n",
    "#print(\"Columns in first file:\", df1.columns)\n",
    "print(\"Columns in first file:\", df1.columns)\n",
    "print(\"Columns in second file:\", df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12239476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aad28cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  NO3-        Lat        Lon station_id\n",
      "0  2000-01-02  4.50  47.766666  16.766666      ID269\n",
      "1  2000-01-08  0.36  47.766666  16.766666      ID269\n",
      "2  2000-01-14  0.70  47.766666  16.766666      ID269\n",
      "3  2000-01-20  0.49  47.766666  16.766666      ID269\n",
      "4  2000-01-26  0.39  47.766666  16.766666      ID269\n",
      "5  2000-02-01  0.79  47.766666  16.766666      ID269\n",
      "6  2000-02-06  0.67  47.766666  16.766666      ID269\n",
      "7  2000-02-12  1.40  47.766666  16.766666      ID269\n",
      "8  2000-02-18  0.44  47.766666  16.766666      ID269\n",
      "9  2000-02-24  2.00  47.766666  16.766666      ID269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the modified CSV files into pandas DataFrames\n",
    "df1 = pd.read_csv('/Users/guloy/Desktop/modified_file1.csv')\n",
    "df2 = pd.read_csv('/Users/guloy/Desktop/modified_file2.csv')\n",
    "\n",
    "\n",
    "df1.drop('ebas_id', axis=1, inplace=True)\n",
    "df1.drop('EBAS_ID', axis=1, inplace=True)\n",
    "df1.rename(columns={'ID': 'station_id'}, inplace=True)\n",
    "print(df1.head(10))\n",
    "df1.to_csv('/Users/guloy/Desktop/modified_file1.csv', index=False)\n",
    "\n",
    "df2.drop('ebas_id', axis=1, inplace=True)\n",
    "df2.drop('EBAS_ID', axis=1, inplace=True)\n",
    "df2.rename(columns={'ID': 'station_id'}, inplace=True)\n",
    "print(df2.head(10))\n",
    "df2.to_csv('/Users/guloy/Desktop/modified_file2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b151753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_path1 = '/Users/guloy/Desktop/modified_file1.csv'  # Replace with your CSV file path\n",
    "csv_file_path2 = '/Users/guloy/Desktop/modified_file2.csv'\n",
    "csv_data1 = pd.read_csv(csv_file_path1)\n",
    "csv_data2 = pd.read_csv(csv_file_path2)\n",
    "\n",
    "\n",
    "# Read the Excel file\n",
    "excel_file_path = '/Users/guloy/Desktop/BSc Thesis/MASTER_FILE_SITE_ID_TO_USE_25_09_2023.xlsx'  # Replace with your Excel file path\n",
    "excel_data = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Assuming 'Station_Name' in Excel maps to 'ebas_id' in CSV,\n",
    "# and 'ID' in Excel is the station_id to add to the CSV\n",
    "merged_data1 = pd.merge(csv_data, excel_data[['EBAS_ID', 'ID']],\n",
    "                       left_on='ebas_id', right_on='EBAS_ID', how='left')\n",
    "\n",
    "merged_data2 = pd.merge(csv_data, excel_data[['EBAS_ID', 'ID']],\n",
    "                       left_on='ebas_id', right_on='EBAS_ID', how='left')\n",
    "\n",
    "\n",
    "# Save the merged data back to CSV\n",
    "merged_data1.to_csv('/Users/guloy/Desktop/modified_file1.csv', index=False)\n",
    "\n",
    "merged_data2.to_csv('/Users/guloy/Desktop/modified_file2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c39ade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file_path1 = '/Users/guloy/Desktop/modified_file1.csv' \n",
    "csv_file_path2 = '/Users/guloy/Desktop/modified_file2.csv' # Replace with your CSV file path\n",
    "data1 = pd.read_csv(csv_file_path1)\n",
    "data2 = pd.read_csv(csv_file_path2)\n",
    "\n",
    "# Define the mapping from Lat values to station_id\n",
    "lat_to_station_id = {\n",
    "    42.72056: 'ID346',\n",
    "    39.516667: 'ID348',\n",
    "    37.05194: 'ID344',\n",
    "    53.002376: 'ID342',\n",
    "    51.541111: 'ID349',\n",
    "    52.3: 'ID343'\n",
    "}\n",
    "\n",
    "# Update station_id based on Lat values\n",
    "for lat, station_id in lat_to_station_id.items():\n",
    "    data1.loc[data1['Lat'] == lat, 'station_id'] = station_id\n",
    "    data2.loc[data2['Lat'] == lat, 'station_id'] = station_id\n",
    "\n",
    "# Save the updated data back to CSV\n",
    "data1.to_csv('/Users/guloy/Desktop/modified_file1.csv', index=False)\n",
    "data2.to_csv('/Users/guloy/Desktop/modified_file1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "264db63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  SO42-        Lat        Lon station_id\n",
      "0  2012-06-16  0.291  40.384444  44.260583      ID308\n",
      "1  2012-06-17  0.248  40.384444  44.260583      ID308\n",
      "2  2012-06-18  0.058  40.384444  44.260583      ID308\n",
      "3  2012-06-19  0.122  40.384444  44.260583      ID308\n",
      "4  2012-06-20  0.000  40.384444  44.260583      ID308\n",
      "5  2012-06-21  0.155  40.384444  44.260583      ID308\n",
      "6  2012-06-25  0.321  40.384444  44.260583      ID308\n",
      "7  2012-06-26  0.295  40.384444  44.260583      ID308\n",
      "8  2012-06-27  0.226  40.384444  44.260583      ID308\n",
      "9  2012-06-28  0.064  40.384444  44.260583      ID308\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV files\n",
    "df1 = pd.read_csv('/Users/guloy/Desktop/modified_file1.csv')  # Replace with your first file path\n",
    "df2 = pd.read_csv('/Users/guloy/Desktop/modified_file2.csv')  # Replace with your second file path\n",
    "\n",
    "# Merging the two dataframes on 'Date' and 'station_id' using an outer join\n",
    "merged_df = pd.merge(df1, df2, on=['Date', 'station_id', 'Lat', 'Lon'], how='outer')\n",
    "\n",
    "# Save the merged dataframe to a new CSV file\n",
    "merged_df.to_csv('/Users/guloy/Desktop/merged_file.csv', index=False\n",
    "print(merged_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b057c44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "  station_id        Date  site_latitude  site_longitude  NO3  SO4     NO3-  \\\n",
      "0      ID053  2015-01-01         35.038          33.058  0.0  0.0  0.00008   \n",
      "1      ID053  2015-01-02         35.038          33.058  0.0  0.0  0.00004   \n",
      "2      ID053  2015-01-03         35.038          33.058  0.0  0.0  0.00001   \n",
      "3      ID053  2015-01-04         35.038          33.058  0.0  0.0  0.00001   \n",
      "4      ID053  2015-01-05         35.038          33.058  0.0  0.0  0.00003   \n",
      "\n",
      "     SO42-  elevation_1k  elevation_yNA_bNA  ...  industrial_b1000  \\\n",
      "0  0.00060             0                0.0  ...               0.0   \n",
      "1  0.00016             0                0.0  ...               0.0   \n",
      "2  0.00024             0                0.0  ...               0.0   \n",
      "3  0.00049             0                0.0  ...               0.0   \n",
      "4  0.00017             0                0.0  ...               0.0   \n",
      "\n",
      "   industrial_b500  roads_rails_b1000  roads_rails_b500  urban_green_b1000  \\\n",
      "0              0.0                0.0               0.0                0.0   \n",
      "1              0.0                0.0               0.0                0.0   \n",
      "2              0.0                0.0               0.0                0.0   \n",
      "3              0.0                0.0               0.0                0.0   \n",
      "4              0.0                0.0               0.0                0.0   \n",
      "\n",
      "   urban_green_b500  population_density_b1000  population_density_b500  \\\n",
      "0               0.0                       0.0                      0.0   \n",
      "1               0.0                       0.0                      0.0   \n",
      "2               0.0                       0.0                      0.0   \n",
      "3               0.0                       0.0                      0.0   \n",
      "4               0.0                       0.0                      0.0   \n",
      "\n",
      "   population_density_b501  population_density  \n",
      "0                      0.0                 0.0  \n",
      "1                      0.0                 0.0  \n",
      "2                      0.0                 0.0  \n",
      "3                      0.0                 0.0  \n",
      "4                      0.0                 0.0  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4f842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
